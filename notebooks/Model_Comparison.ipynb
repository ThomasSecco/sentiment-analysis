{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/Model_Comparison.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained models and test data\n",
    "X_test_tfidf = joblib.load(\"../data/processed/X_test_tfidf.pkl\")\n",
    "y_test = pd.read_csv(\"../data/processed/y_test.csv\").values.ravel()\n",
    "\n",
    "# Load different models\n",
    "models = {\n",
    "    \"Logistic Regression\": joblib.load(\"../models/logistic_regression_model.pkl\"),\n",
    "    \"SVM\": joblib.load(\"../models/svm_model.pkl\"),\n",
    "    \"Random Forest\": joblib.load(\"../models/random_forest_model.pkl\"),\n",
    "    \"LSTM\": joblib.load(\"../models/lstm_model.h5\"),\n",
    "    \"BERT\": joblib.load(\"../models/bert_model.pkl\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluate each model and store results\n",
    "results = []\n",
    "for model_name, model in models.items():\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, X_test_tfidf, y_test)\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display results in a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "\n",
    "# Plotting performance metrics for comparison\n",
    "results_df.set_index(\"Model\").plot(kind=\"bar\", figsize=(12, 6))\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion Matrix for the best model\n",
    "best_model_name = results_df.loc[results_df['F1 Score'].idxmax(), 'Model']\n",
    "best_model = models[best_model_name]\n",
    "y_pred_best = best_model.predict(X_test_tfidf)\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "# Confusion matrix visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f\"Confusion Matrix - {best_model_name}\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
